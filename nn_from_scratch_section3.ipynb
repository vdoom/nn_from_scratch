{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7116e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5721c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(array: ndarray,\n",
    "                      array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "        '''\n",
    "        Two ndarrays should have the same shape;\n",
    "        instead, first ndarray's shape is {0}\n",
    "        and second ndarray's shape is {1}.\n",
    "        '''.format(tuple(array_grad.shape), tuple(array.shape))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c4740",
   "metadata": {},
   "source": [
    "# Operation and ParamOperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2eec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    '''\n",
    "    Base class for an \"operation\" in a neural network.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_: ndarray):\n",
    "        '''\n",
    "        Stores input in the self._input instance variable\n",
    "        Calls the self._output() function.\n",
    "        '''\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calls the self._input_grad() function.\n",
    "        Checks that the appropriate shapes match.\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        The _output method must be defined for each Operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        The _input_grad method must be defined for each Operation\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7917b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    '''\n",
    "    An Operation with parameters.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, param: ndarray) -> ndarray:\n",
    "        '''\n",
    "        The ParamOperation method\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calls self._input_grad and self._param_grad.\n",
    "        Checks appropriate shapes.\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Every subclass of ParamOperation must implement _param_grad.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b62a7f",
   "metadata": {},
   "source": [
    "# Specific Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e99bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    '''\n",
    "    Weight multiplication operation for a neural network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        '''\n",
    "        Initialize Operation with self.param = W.\n",
    "        '''\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output.\n",
    "        '''\n",
    "        return np.dot(self.input_, self.param)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute input gradient.\n",
    "        '''\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:\n",
    "        '''\n",
    "        Compute parameter gradient.\n",
    "        '''        \n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8772d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    '''\n",
    "    Compute bias addition.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 B: ndarray):\n",
    "        '''\n",
    "        Initialize Operation with self.param = B.\n",
    "        Check appropriate shape.\n",
    "        '''\n",
    "        assert B.shape[0] == 1\n",
    "        \n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output.\n",
    "        '''\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute input gradient.\n",
    "        '''\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute parameter gradient.\n",
    "        '''\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be36dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Sigmoid activation function.\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output.\n",
    "        '''\n",
    "        return 1.0/(1.0+np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute input gradient.\n",
    "        '''\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb916fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    '''\n",
    "    \"Identity\" activation function\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''        \n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''Pass through'''\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''Pass through'''\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff47572",
   "metadata": {},
   "source": [
    "# Layer and Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356bce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    '''\n",
    "    A \"layer\" of neurons in a neural network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 neurons: int):\n",
    "        '''\n",
    "        The number of \"neurons\" roughly corresponds to the \"breadth\" of the layer\n",
    "        '''\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    def _setup_layer(self, num_in: int) -> None:\n",
    "        '''\n",
    "        The _setup_layer function must be implemented for each layer\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes input forward through a series of operations\n",
    "        ''' \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        for operation in self.operations:\n",
    "\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes output_grad backward through a series of operations\n",
    "        Checks appropriate shapes\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        '''\n",
    "        Extracts the _param_grads from a layer's operations\n",
    "        '''\n",
    "\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> ndarray:\n",
    "        '''\n",
    "        Extracts the _params from a layer's operations\n",
    "        '''\n",
    "\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13124f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    '''\n",
    "    A fully connected layer which inherits from \"Layer\"\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 neurons: int,\n",
    "                 activation: Operation = Sigmoid()):\n",
    "        '''\n",
    "        Requires an activation function upon initialization\n",
    "        '''\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        '''\n",
    "        Defines the operations of a fully connected layer.\n",
    "        '''\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "\n",
    "        # bias\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a9b7a",
   "metadata": {},
   "source": [
    "# Loss and MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a32f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''\n",
    "    The \"loss\" of a neural network\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Pass'''\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        '''\n",
    "        Computes the actual loss value\n",
    "        '''\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self) -> ndarray:\n",
    "        '''\n",
    "        Computes gradient of the loss value with respect to the input to the loss function\n",
    "        '''\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        Every subclass of \"Loss\" must implement the _output function.\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        Every subclass of \"Loss\" must implement the _input_grad function.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0009a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        Computes the per-observation squared error loss\n",
    "        '''\n",
    "        loss = (\n",
    "            np.sum(np.power(self.prediction - self.target, 2)) / \n",
    "            self.prediction.shape[0]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        Computes the loss gradient with respect to the input for MSE loss\n",
    "        '''        \n",
    "\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5114db7",
   "metadata": {},
   "source": [
    "# NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee9ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    '''\n",
    "    The class for a neural network.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[Layer],\n",
    "                 loss: Loss,\n",
    "                 seed: int = 1) -> None:\n",
    "        '''\n",
    "        Neural networks need layers, and a loss.\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)        \n",
    "\n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes data forward through a series of layers.\n",
    "        '''\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, loss_grad: ndarray) -> None:\n",
    "        '''\n",
    "        Passes data backward through a series of layers.\n",
    "        '''\n",
    "\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train_batch(self,\n",
    "                    x_batch: ndarray,\n",
    "                    y_batch: ndarray) -> float:\n",
    "        '''\n",
    "        Passes data forward through the layers.\n",
    "        Computes the loss.\n",
    "        Passes data backward through the layers.\n",
    "        '''\n",
    "        \n",
    "        predictions = self.forward(x_batch)\n",
    "\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "\n",
    "        self.backward(self.loss.backward())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        '''\n",
    "        Gets the parameters for the network.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        '''\n",
    "        Gets the gradient of the loss with respect to the parameters for the network.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d30eb",
   "metadata": {},
   "source": [
    "# Optimizer and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdf4ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''\n",
    "    Base class for a neural network optimizer.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01):\n",
    "        '''\n",
    "        Every optimizer must have an initial learning rate.\n",
    "        '''\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self) -> None:\n",
    "        '''\n",
    "        Every optimizer must implement the \"step\" function.\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4eb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    '''\n",
    "    Stochasitc gradient descent optimizer.\n",
    "    '''    \n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__(lr)\n",
    "\n",
    "    def step(self):\n",
    "        '''\n",
    "        For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment \n",
    "        based on the learning rate.\n",
    "        '''\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ac1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
